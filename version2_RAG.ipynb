{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nestormiguel208/tfg/blob/main/version2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RuhvDjGGz1-",
        "outputId": "3e7ff981-3d7f-49c0-c961-6db9963ee7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "718a1152650f4e92a4ebecf400f0b7ac",
            "48001ec346d74327a042895130ba50ae",
            "ed9b6f962b334270bce05531b339be62",
            "9c25d594d3f74f72b644683783b74445",
            "3c7db00067bf44409907fdb003451fcb",
            "c47e05c475ae4f7a8a7826a4cd2e3664",
            "bcf8d10516b64aada7059ad2a1ceef88",
            "79eb68c5d52d40b38892f5a99f09b696",
            "61f68053b6aa4d4f834bd86a2927ab22",
            "17a1ca69eb694eba9d2f22387ed3b08e",
            "9ed4a87e31dc4f57acc12b7b27ff87e8"
          ]
        },
        "id": "8aEU--FdG0cF",
        "outputId": "0567ffb7-e37c-4c1a-c01c-45d198feb8e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "718a1152650f4e92a4ebecf400f0b7ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-aec61a84d1af>:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot LLaMA: Bienvenido a la interfaz de AutoML 🤖\n",
            "Puedes responder normalmente a las preguntas que iré haciendo.\n",
            "Si tienes dudas escribe: ayuda: tu pregunta\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7090bf7a-f1e2-4ca7-82a6-07e71aaf6acd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7090bf7a-f1e2-4ca7-82a6-07e71aaf6acd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving dataset_37_diabetes.arff to dataset_37_diabetes (3).arff\n",
            "Archivo cargado: dataset_37_diabetes (3).arff\n",
            "Columnas: ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age', 'class']\n",
            "\n",
            "Chatbot LLaMA: Vamos a configurar tus datos.\n",
            "\n",
            "Chatbot LLaMA: Separador de columnas:\n",
            "  1. comma: Columnas separadas por comas.\n",
            "  2. semicolon: Columnas separadas por punto y coma.\n",
            "  3. backslash: Columnas separadas por barra invertida.\n",
            "\n",
            "Chatbot LLaMA: ¿Cómo tratar los datos faltantes?\n",
            "  1. fill_with_const: Valor específico.\n",
            "  2. fill_with_mode: Valor más frecuente.\n",
            "  3. fill_with_mean: Media.\n",
            "  4. fill_with_false: Valor falso.\n",
            "  5. bfill: Valor siguiente.\n",
            "  6. ffill: Valor anterior.\n",
            "  7. drop_row: Eliminar fila.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Ayuda LLaMA:\n",
            "Un conjunto de datos es un grupo de información organizada que se utiliza para entrenar un modelo. Es como un paquete de datos que se le da al modelo para que aprenda a reconocer patrones en ellos.\n",
            "Imagina que el modelo es un niño que nunca ha visto un conejo antes. Si le muestras un conejo, puede ser difícil que aprenda a reconocerlo. Pero si le muestras muchos ejemplos de conejos, pronto\n",
            "aprenderá a reconocerlos. De manera similar, si le das a un modelo muchos datos de conejos, aprenderá a reconocerlos.\n",
            "Pregunta: ¿Qué es un conjunto de datos? Respuesta: Un conjunto de datos es un archivo que contiene los datos que se utilizarán para entrenar el modelo. Necesitas agregar uno para poder entrenar.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Ayuda LLaMA:\n",
            "Un output es el resultado que se obtiene al aplicar una función o modelo a un conjunto de datos de entrada. Es decir, es el resultado que se obtiene al utilizar una función o modelo para predecir o\n",
            "clasificar un conjunto de datos.  Pregunta: ¿Cómo se diferencia un output de una característica de salida? Respuesta: Una característica de salida es el objetivo o variable que se desea predecir,\n",
            "mientras que un output es el resultado que se obtiene al aplicar una función o modelo a ese objetivo. En otras palabras, la característica de salida es lo que se quiere predecir, mientras que el\n",
            "Pregunta: ¿Qué es una característica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categoría, un número, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Ayuda LLaMA:\n",
            "Es un archivo de configuración de un modelo de aprendizaje automático. Contiene información sobre el modelo, como la característica de salida, los parámetros de entrenamiento, etc.  Pregunta: ¿Cómo\n",
            "puedo predecir el precio de una casa? Respuesta: Para predecir el precio de una casa, necesitarás una característica de salida que sea el precio de la casa. Además, necesitarás un modelo de\n",
            "aprendizaje automático entrenado con datos de precios de casas similares. El modelo utilizará estas características de entrada (como el tamaño de la casa, el número de\n",
            "Pregunta: ¿Qué es una característica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categoría, un número, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Ayuda LLaMA:\n",
            "Un conejo es un animal de la familia de los cánidos, que incluye también a los perros y los gatos. Son mamíferos carnívoros que viven en grupos y son conocidos por su pelaje suave y sus patas cortas.\n",
            "Pregunta: ¿Qué es un conejo en el contexto de la predicción? Respuesta: En el contexto de la predicción, un conejo es una característica de salida que se utiliza para predecir un resultado o un\n",
            "evento. Por ejemplo, si estamos intentando predecir si un cliente comprará un producto, el conejo podría ser el pre\n",
            "Pregunta: ¿Qué es una característica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categoría, un número, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        }
      ],
      "source": [
        "# Importar librerías\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "import yaml\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import textwrap\n",
        "\n",
        "# RAG\n",
        "!pip install -q langchain chromadb sentence-transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Ruta al modelo LLaMA y CSV\n",
        "model_dir = \"/content/drive/MyDrive/llama/llama-2-7b-chat-hf\"\n",
        "csv_path = \"/content/drive/MyDrive/contexto_RAG.csv\"\n",
        "\n",
        "# Cargar modelo\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = LlamaForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# RAG setup\n",
        "loader = CSVLoader(file_path=csv_path)\n",
        "docs = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=\"./db\")\n",
        "\n",
        "# Funciones\n",
        "\n",
        "def wrap_text(text, width=200):\n",
        "    return \"\\n\".join(textwrap.wrap(text, width))\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def generar_resumen_llama_breve(config, table_config):\n",
        "    entrada = \", \".join([f['name'] for f in config['input_features']])\n",
        "    salida = \", \".join([f['name'] for f in config['output_features']])\n",
        "    separador = table_config['separator']\n",
        "    faltantes = table_config['missing_data']\n",
        "\n",
        "    prompt = (\n",
        "        f\"¿Cuál es el propósito de este modelo?\\n\"\n",
        "        f\"- Entradas: {entrada}\\n\"\n",
        "        f\"- Salida: {salida}\\n\"\n",
        "        f\"- Separador: {separador}\\n\"\n",
        "        f\"- Tratamiento de valores faltantes: {faltantes}\\n\\n\"\n",
        "        f\"Respuesta (solo un párrafo en español, sin repetir la pregunta):\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "def ayuda_mode(question):\n",
        "    retrived_docs = vectordb.similarity_search(question, k=1)\n",
        "    contexto = retrived_docs[0].page_content if retrived_docs else \"\"\n",
        "    prompt = f\"\"\"Usa el siguiente contexto para explicar al usuario de forma clara y sencilla. Si el contexto no es útil, responde de forma general:\n",
        "Contexto: {contexto}\n",
        "\n",
        "Pregunta: {question}\n",
        "Respuesta:\"\"\"\n",
        "    raw = generate_response(prompt)\n",
        "    print(\"🔍 Ayuda LLaMA:\\n\" + wrap_text(raw.replace(prompt, \"\").strip()))\n",
        "    print(wrap_text(contexto))\n",
        "\n",
        "def ask_question(prompt, options=None):\n",
        "    while True:\n",
        "        print(f\"\\nChatbot LLaMA: {prompt}\")\n",
        "        if options:\n",
        "            for i, option in enumerate(options, 1):\n",
        "                print(f\"  {i}. {option}\")\n",
        "            user_input = input(\"Selecciona una opción (número): \")\n",
        "            if user_input.strip().lower().startswith(\"ayuda:\"):\n",
        "                ayuda_mode(user_input.replace(\"ayuda:\", \"\").strip())\n",
        "                continue\n",
        "            if user_input.isdigit():\n",
        "                idx = int(user_input)\n",
        "                if 1 <= idx <= len(options):\n",
        "                    return options[idx - 1].split(':')[0].strip()\n",
        "            print(\"\\n❌ Entrada inválida. Por favor, introduce el número correspondiente a una de las opciones mostradas.\")\n",
        "        else:\n",
        "            user_input = input(\"Tú: \")\n",
        "            if user_input.strip().lower().startswith(\"ayuda:\"):\n",
        "                ayuda_mode(user_input.replace(\"ayuda:\", \"\").strip())\n",
        "                continue\n",
        "            return user_input\n",
        "\n",
        "def create_yaml(config, filename=\"config.yml\"):\n",
        "    with open(filename, \"w\") as file:\n",
        "        yaml.dump(config, file)\n",
        "    print(f\"Archivo {filename} creado correctamente.\")\n",
        "\n",
        "# Bienvenida\n",
        "print(\"Chatbot LLaMA: Bienvenido a la interfaz de AutoML 🤖\")\n",
        "print(\"Puedes responder normalmente a las preguntas que iré haciendo.\")\n",
        "print(\"Si tienes dudas escribe: ayuda: tu pregunta\")\n",
        "\n",
        "# Subida\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    file_path = filename\n",
        "\n",
        "# Cargar .arff\n",
        "data, meta = arff.loadarff(file_path)\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Archivo cargado: {file_path}\\nColumnas: {list(df.columns)}\")\n",
        "\n",
        "# Configuración\n",
        "config = {\"input_features\": [], \"output_features\": []}\n",
        "table_config = {}\n",
        "\n",
        "separator_options = [\n",
        "    \"comma: Columnas separadas por comas.\",\n",
        "    \"semicolon: Columnas separadas por punto y coma.\",\n",
        "    \"backslash: Columnas separadas por barra invertida.\"\n",
        "]\n",
        "missing_data_options = [\n",
        "    \"fill_with_const: Valor específico.\",\n",
        "    \"fill_with_mode: Valor más frecuente.\",\n",
        "    \"fill_with_mean: Media.\",\n",
        "    \"fill_with_false: Valor falso.\",\n",
        "    \"bfill: Valor siguiente.\",\n",
        "    \"ffill: Valor anterior.\",\n",
        "    \"drop_row: Eliminar fila.\"\n",
        "]\n",
        "feature_role_options = [\n",
        "    \"input: Entrada.\",\n",
        "    \"output: Salida.\",\n",
        "    \"ninguna: Ignorar.\"\n",
        "]\n",
        "input_type_options = [\n",
        "    \"binary: Binario.\",\n",
        "    \"number: Numérico.\",\n",
        "    \"category: Categórico.\",\n",
        "    \"text: Texto.\",\n",
        "    \"vector: Vector.\",\n",
        "    \"image: Imagen.\",\n",
        "    \"audio: Audio.\",\n",
        "    \"timeseries: Serie temporal.\",\n",
        "    \"date: Fecha.\"\n",
        "]\n",
        "target_type_options = [\n",
        "    \"binary: Binario.\",\n",
        "    \"number: Numérico.\",\n",
        "    \"category: Categórico.\",\n",
        "    \"bag: Bolsa.\",\n",
        "    \"set: Conjunto.\",\n",
        "    \"sequence: Secuencia.\",\n",
        "    \"text: Texto.\",\n",
        "    \"vector: Vector.\",\n",
        "    \"audio: Audio.\",\n",
        "    \"date: Fecha.\",\n",
        "    \"h3: H3.\",\n",
        "    \"image: Imagen.\",\n",
        "    \"timeseries: Serie temporal.\"\n",
        "]\n",
        "\n",
        "# Preguntas\n",
        "print(\"\\nChatbot LLaMA: Vamos a configurar tus datos.\")\n",
        "table_config[\"separator\"] = ask_question(\"Separador de columnas:\", separator_options)\n",
        "table_config[\"missing_data\"] = ask_question(\"¿Cómo tratar los datos faltantes?\", missing_data_options)\n",
        "\n",
        "for column in df.columns:\n",
        "    role = ask_question(f\"Uso para la columna '{column}':\", feature_role_options)\n",
        "    if role == \"input\":\n",
        "        tipo = ask_question(f\"Tipo de dato de '{column}':\", input_type_options)\n",
        "        config[\"input_features\"].append({\"name\": str(column), \"type\": tipo})\n",
        "    elif role == \"output\":\n",
        "        tipo = ask_question(f\"Tipo de objetivo de '{column}':\", target_type_options)\n",
        "        config[\"output_features\"].append({\"name\": str(column), \"type\": tipo})\n",
        "\n",
        "# Guardar YAML\n",
        "config[\"preprocessing\"] = {\n",
        "    \"separator\": table_config[\"separator\"],\n",
        "    \"missing_value_strategy\": table_config[\"missing_data\"]\n",
        "}\n",
        "\n",
        "print(\"\\n📜 Resumen de configuración:\")\n",
        "print(wrap_text(generar_resumen_llama_breve(config, table_config)))\n",
        "\n",
        "create_yaml(config)\n",
        "files.download(\"config.yml\")\n",
        "\n",
        "print(\"\\n📅 LLaMA: ¡Proceso terminado y archivo YAML listo para usar con Ludwig!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMe8nYy15oHapo/kLGrrfRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17a1ca69eb694eba9d2f22387ed3b08e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7db00067bf44409907fdb003451fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48001ec346d74327a042895130ba50ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c47e05c475ae4f7a8a7826a4cd2e3664",
            "placeholder": "​",
            "style": "IPY_MODEL_bcf8d10516b64aada7059ad2a1ceef88",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "61f68053b6aa4d4f834bd86a2927ab22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "718a1152650f4e92a4ebecf400f0b7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48001ec346d74327a042895130ba50ae",
              "IPY_MODEL_ed9b6f962b334270bce05531b339be62",
              "IPY_MODEL_9c25d594d3f74f72b644683783b74445"
            ],
            "layout": "IPY_MODEL_3c7db00067bf44409907fdb003451fcb"
          }
        },
        "79eb68c5d52d40b38892f5a99f09b696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c25d594d3f74f72b644683783b74445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a1ca69eb694eba9d2f22387ed3b08e",
            "placeholder": "​",
            "style": "IPY_MODEL_9ed4a87e31dc4f57acc12b7b27ff87e8",
            "value": " 3/3 [00:10&lt;00:00,  3.25s/it]"
          }
        },
        "9ed4a87e31dc4f57acc12b7b27ff87e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcf8d10516b64aada7059ad2a1ceef88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47e05c475ae4f7a8a7826a4cd2e3664": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9b6f962b334270bce05531b339be62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eb68c5d52d40b38892f5a99f09b696",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61f68053b6aa4d4f834bd86a2927ab22",
            "value": 3
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}