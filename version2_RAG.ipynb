{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nestormiguel208/tfg/blob/main/version2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RuhvDjGGz1-",
        "outputId": "3e7ff981-3d7f-49c0-c961-6db9963ee7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "718a1152650f4e92a4ebecf400f0b7ac",
            "48001ec346d74327a042895130ba50ae",
            "ed9b6f962b334270bce05531b339be62",
            "9c25d594d3f74f72b644683783b74445",
            "3c7db00067bf44409907fdb003451fcb",
            "c47e05c475ae4f7a8a7826a4cd2e3664",
            "bcf8d10516b64aada7059ad2a1ceef88",
            "79eb68c5d52d40b38892f5a99f09b696",
            "61f68053b6aa4d4f834bd86a2927ab22",
            "17a1ca69eb694eba9d2f22387ed3b08e",
            "9ed4a87e31dc4f57acc12b7b27ff87e8"
          ]
        },
        "id": "8aEU--FdG0cF",
        "outputId": "0567ffb7-e37c-4c1a-c01c-45d198feb8e9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "718a1152650f4e92a4ebecf400f0b7ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-2-aec61a84d1af>:34: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chatbot LLaMA: Bienvenido a la interfaz de AutoML ü§ñ\n",
            "Puedes responder normalmente a las preguntas que ir√© haciendo.\n",
            "Si tienes dudas escribe: ayuda: tu pregunta\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7090bf7a-f1e2-4ca7-82a6-07e71aaf6acd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7090bf7a-f1e2-4ca7-82a6-07e71aaf6acd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving dataset_37_diabetes.arff to dataset_37_diabetes (3).arff\n",
            "Archivo cargado: dataset_37_diabetes (3).arff\n",
            "Columnas: ['preg', 'plas', 'pres', 'skin', 'insu', 'mass', 'pedi', 'age', 'class']\n",
            "\n",
            "Chatbot LLaMA: Vamos a configurar tus datos.\n",
            "\n",
            "Chatbot LLaMA: Separador de columnas:\n",
            "  1. comma: Columnas separadas por comas.\n",
            "  2. semicolon: Columnas separadas por punto y coma.\n",
            "  3. backslash: Columnas separadas por barra invertida.\n",
            "\n",
            "Chatbot LLaMA: ¬øC√≥mo tratar los datos faltantes?\n",
            "  1. fill_with_const: Valor espec√≠fico.\n",
            "  2. fill_with_mode: Valor m√°s frecuente.\n",
            "  3. fill_with_mean: Media.\n",
            "  4. fill_with_false: Valor falso.\n",
            "  5. bfill: Valor siguiente.\n",
            "  6. ffill: Valor anterior.\n",
            "  7. drop_row: Eliminar fila.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Ayuda LLaMA:\n",
            "Un conjunto de datos es un grupo de informaci√≥n organizada que se utiliza para entrenar un modelo. Es como un paquete de datos que se le da al modelo para que aprenda a reconocer patrones en ellos.\n",
            "Imagina que el modelo es un ni√±o que nunca ha visto un conejo antes. Si le muestras un conejo, puede ser dif√≠cil que aprenda a reconocerlo. Pero si le muestras muchos ejemplos de conejos, pronto\n",
            "aprender√° a reconocerlos. De manera similar, si le das a un modelo muchos datos de conejos, aprender√° a reconocerlos.\n",
            "Pregunta: ¬øQu√© es un conjunto de datos? Respuesta: Un conjunto de datos es un archivo que contiene los datos que se utilizar√°n para entrenar el modelo. Necesitas agregar uno para poder entrenar.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Ayuda LLaMA:\n",
            "Un output es el resultado que se obtiene al aplicar una funci√≥n o modelo a un conjunto de datos de entrada. Es decir, es el resultado que se obtiene al utilizar una funci√≥n o modelo para predecir o\n",
            "clasificar un conjunto de datos.  Pregunta: ¬øC√≥mo se diferencia un output de una caracter√≠stica de salida? Respuesta: Una caracter√≠stica de salida es el objetivo o variable que se desea predecir,\n",
            "mientras que un output es el resultado que se obtiene al aplicar una funci√≥n o modelo a ese objetivo. En otras palabras, la caracter√≠stica de salida es lo que se quiere predecir, mientras que el\n",
            "Pregunta: ¬øQu√© es una caracter√≠stica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categor√≠a, un n√∫mero, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Ayuda LLaMA:\n",
            "Es un archivo de configuraci√≥n de un modelo de aprendizaje autom√°tico. Contiene informaci√≥n sobre el modelo, como la caracter√≠stica de salida, los par√°metros de entrenamiento, etc.  Pregunta: ¬øC√≥mo\n",
            "puedo predecir el precio de una casa? Respuesta: Para predecir el precio de una casa, necesitar√°s una caracter√≠stica de salida que sea el precio de la casa. Adem√°s, necesitar√°s un modelo de\n",
            "aprendizaje autom√°tico entrenado con datos de precios de casas similares. El modelo utilizar√° estas caracter√≠sticas de entrada (como el tama√±o de la casa, el n√∫mero de\n",
            "Pregunta: ¬øQu√© es una caracter√≠stica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categor√≠a, un n√∫mero, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Ayuda LLaMA:\n",
            "Un conejo es un animal de la familia de los c√°nidos, que incluye tambi√©n a los perros y los gatos. Son mam√≠feros carn√≠voros que viven en grupos y son conocidos por su pelaje suave y sus patas cortas.\n",
            "Pregunta: ¬øQu√© es un conejo en el contexto de la predicci√≥n? Respuesta: En el contexto de la predicci√≥n, un conejo es una caracter√≠stica de salida que se utiliza para predecir un resultado o un\n",
            "evento. Por ejemplo, si estamos intentando predecir si un cliente comprar√° un producto, el conejo podr√≠a ser el pre\n",
            "Pregunta: ¬øQu√© es una caracter√≠stica de salida? Respuesta: Es el objetivo o la variable que deseas predecir. Puede ser una categor√≠a, un n√∫mero, etc.\n",
            "\n",
            "Chatbot LLaMA: Uso para la columna 'preg':\n",
            "  1. input: Entrada.\n",
            "  2. output: Salida.\n",
            "  3. ninguna: Ignorar.\n"
          ]
        }
      ],
      "source": [
        "# Importar librer√≠as\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "import yaml\n",
        "from scipy.io import arff\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import textwrap\n",
        "\n",
        "# RAG\n",
        "!pip install -q langchain chromadb sentence-transformers\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Ruta al modelo LLaMA y CSV\n",
        "model_dir = \"/content/drive/MyDrive/llama/llama-2-7b-chat-hf\"\n",
        "csv_path = \"/content/drive/MyDrive/contexto_RAG.csv\"\n",
        "\n",
        "# Cargar modelo\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "model = LlamaForCausalLM.from_pretrained(model_dir, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "\n",
        "# RAG setup\n",
        "loader = CSVLoader(file_path=csv_path)\n",
        "docs = loader.load()\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma.from_documents(documents=documents, embedding=embedding, persist_directory=\"./db\")\n",
        "\n",
        "# Funciones\n",
        "\n",
        "def wrap_text(text, width=200):\n",
        "    return \"\\n\".join(textwrap.wrap(text, width))\n",
        "\n",
        "def generate_response(prompt):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256).to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "def generar_resumen_llama_breve(config, table_config):\n",
        "    entrada = \", \".join([f['name'] for f in config['input_features']])\n",
        "    salida = \", \".join([f['name'] for f in config['output_features']])\n",
        "    separador = table_config['separator']\n",
        "    faltantes = table_config['missing_data']\n",
        "\n",
        "    prompt = (\n",
        "        f\"¬øCu√°l es el prop√≥sito de este modelo?\\n\"\n",
        "        f\"- Entradas: {entrada}\\n\"\n",
        "        f\"- Salida: {salida}\\n\"\n",
        "        f\"- Separador: {separador}\\n\"\n",
        "        f\"- Tratamiento de valores faltantes: {faltantes}\\n\\n\"\n",
        "        f\"Respuesta (solo un p√°rrafo en espa√±ol, sin repetir la pregunta):\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response.replace(prompt, \"\").strip()\n",
        "\n",
        "def ayuda_mode(question):\n",
        "    retrived_docs = vectordb.similarity_search(question, k=1)\n",
        "    contexto = retrived_docs[0].page_content if retrived_docs else \"\"\n",
        "    prompt = f\"\"\"Usa el siguiente contexto para explicar al usuario de forma clara y sencilla. Si el contexto no es √∫til, responde de forma general:\n",
        "Contexto: {contexto}\n",
        "\n",
        "Pregunta: {question}\n",
        "Respuesta:\"\"\"\n",
        "    raw = generate_response(prompt)\n",
        "    print(\"üîç Ayuda LLaMA:\\n\" + wrap_text(raw.replace(prompt, \"\").strip()))\n",
        "    print(wrap_text(contexto))\n",
        "\n",
        "def ask_question(prompt, options=None):\n",
        "    while True:\n",
        "        print(f\"\\nChatbot LLaMA: {prompt}\")\n",
        "        if options:\n",
        "            for i, option in enumerate(options, 1):\n",
        "                print(f\"  {i}. {option}\")\n",
        "            user_input = input(\"Selecciona una opci√≥n (n√∫mero): \")\n",
        "            if user_input.strip().lower().startswith(\"ayuda:\"):\n",
        "                ayuda_mode(user_input.replace(\"ayuda:\", \"\").strip())\n",
        "                continue\n",
        "            if user_input.isdigit():\n",
        "                idx = int(user_input)\n",
        "                if 1 <= idx <= len(options):\n",
        "                    return options[idx - 1].split(':')[0].strip()\n",
        "            print(\"\\n‚ùå Entrada inv√°lida. Por favor, introduce el n√∫mero correspondiente a una de las opciones mostradas.\")\n",
        "        else:\n",
        "            user_input = input(\"T√∫: \")\n",
        "            if user_input.strip().lower().startswith(\"ayuda:\"):\n",
        "                ayuda_mode(user_input.replace(\"ayuda:\", \"\").strip())\n",
        "                continue\n",
        "            return user_input\n",
        "\n",
        "def create_yaml(config, filename=\"config.yml\"):\n",
        "    with open(filename, \"w\") as file:\n",
        "        yaml.dump(config, file)\n",
        "    print(f\"Archivo {filename} creado correctamente.\")\n",
        "\n",
        "# Bienvenida\n",
        "print(\"Chatbot LLaMA: Bienvenido a la interfaz de AutoML ü§ñ\")\n",
        "print(\"Puedes responder normalmente a las preguntas que ir√© haciendo.\")\n",
        "print(\"Si tienes dudas escribe: ayuda: tu pregunta\")\n",
        "\n",
        "# Subida\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    file_path = filename\n",
        "\n",
        "# Cargar .arff\n",
        "data, meta = arff.loadarff(file_path)\n",
        "df = pd.DataFrame(data)\n",
        "print(f\"Archivo cargado: {file_path}\\nColumnas: {list(df.columns)}\")\n",
        "\n",
        "# Configuraci√≥n\n",
        "config = {\"input_features\": [], \"output_features\": []}\n",
        "table_config = {}\n",
        "\n",
        "separator_options = [\n",
        "    \"comma: Columnas separadas por comas.\",\n",
        "    \"semicolon: Columnas separadas por punto y coma.\",\n",
        "    \"backslash: Columnas separadas por barra invertida.\"\n",
        "]\n",
        "missing_data_options = [\n",
        "    \"fill_with_const: Valor espec√≠fico.\",\n",
        "    \"fill_with_mode: Valor m√°s frecuente.\",\n",
        "    \"fill_with_mean: Media.\",\n",
        "    \"fill_with_false: Valor falso.\",\n",
        "    \"bfill: Valor siguiente.\",\n",
        "    \"ffill: Valor anterior.\",\n",
        "    \"drop_row: Eliminar fila.\"\n",
        "]\n",
        "feature_role_options = [\n",
        "    \"input: Entrada.\",\n",
        "    \"output: Salida.\",\n",
        "    \"ninguna: Ignorar.\"\n",
        "]\n",
        "input_type_options = [\n",
        "    \"binary: Binario.\",\n",
        "    \"number: Num√©rico.\",\n",
        "    \"category: Categ√≥rico.\",\n",
        "    \"text: Texto.\",\n",
        "    \"vector: Vector.\",\n",
        "    \"image: Imagen.\",\n",
        "    \"audio: Audio.\",\n",
        "    \"timeseries: Serie temporal.\",\n",
        "    \"date: Fecha.\"\n",
        "]\n",
        "target_type_options = [\n",
        "    \"binary: Binario.\",\n",
        "    \"number: Num√©rico.\",\n",
        "    \"category: Categ√≥rico.\",\n",
        "    \"bag: Bolsa.\",\n",
        "    \"set: Conjunto.\",\n",
        "    \"sequence: Secuencia.\",\n",
        "    \"text: Texto.\",\n",
        "    \"vector: Vector.\",\n",
        "    \"audio: Audio.\",\n",
        "    \"date: Fecha.\",\n",
        "    \"h3: H3.\",\n",
        "    \"image: Imagen.\",\n",
        "    \"timeseries: Serie temporal.\"\n",
        "]\n",
        "\n",
        "# Preguntas\n",
        "print(\"\\nChatbot LLaMA: Vamos a configurar tus datos.\")\n",
        "table_config[\"separator\"] = ask_question(\"Separador de columnas:\", separator_options)\n",
        "table_config[\"missing_data\"] = ask_question(\"¬øC√≥mo tratar los datos faltantes?\", missing_data_options)\n",
        "\n",
        "for column in df.columns:\n",
        "    role = ask_question(f\"Uso para la columna '{column}':\", feature_role_options)\n",
        "    if role == \"input\":\n",
        "        tipo = ask_question(f\"Tipo de dato de '{column}':\", input_type_options)\n",
        "        config[\"input_features\"].append({\"name\": str(column), \"type\": tipo})\n",
        "    elif role == \"output\":\n",
        "        tipo = ask_question(f\"Tipo de objetivo de '{column}':\", target_type_options)\n",
        "        config[\"output_features\"].append({\"name\": str(column), \"type\": tipo})\n",
        "\n",
        "# Guardar YAML\n",
        "config[\"preprocessing\"] = {\n",
        "    \"separator\": table_config[\"separator\"],\n",
        "    \"missing_value_strategy\": table_config[\"missing_data\"]\n",
        "}\n",
        "\n",
        "print(\"\\nüìú Resumen de configuraci√≥n:\")\n",
        "print(wrap_text(generar_resumen_llama_breve(config, table_config)))\n",
        "\n",
        "create_yaml(config)\n",
        "files.download(\"config.yml\")\n",
        "\n",
        "print(\"\\nüìÖ LLaMA: ¬°Proceso terminado y archivo YAML listo para usar con Ludwig!\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMe8nYy15oHapo/kLGrrfRR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17a1ca69eb694eba9d2f22387ed3b08e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7db00067bf44409907fdb003451fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48001ec346d74327a042895130ba50ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c47e05c475ae4f7a8a7826a4cd2e3664",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bcf8d10516b64aada7059ad2a1ceef88",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "61f68053b6aa4d4f834bd86a2927ab22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "718a1152650f4e92a4ebecf400f0b7ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48001ec346d74327a042895130ba50ae",
              "IPY_MODEL_ed9b6f962b334270bce05531b339be62",
              "IPY_MODEL_9c25d594d3f74f72b644683783b74445"
            ],
            "layout": "IPY_MODEL_3c7db00067bf44409907fdb003451fcb"
          }
        },
        "79eb68c5d52d40b38892f5a99f09b696": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c25d594d3f74f72b644683783b74445": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a1ca69eb694eba9d2f22387ed3b08e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9ed4a87e31dc4f57acc12b7b27ff87e8",
            "value": "‚Äá3/3‚Äá[00:10&lt;00:00,‚Äá‚Äá3.25s/it]"
          }
        },
        "9ed4a87e31dc4f57acc12b7b27ff87e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcf8d10516b64aada7059ad2a1ceef88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c47e05c475ae4f7a8a7826a4cd2e3664": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9b6f962b334270bce05531b339be62": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79eb68c5d52d40b38892f5a99f09b696",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61f68053b6aa4d4f834bd86a2927ab22",
            "value": 3
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}